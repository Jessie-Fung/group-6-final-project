{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83736cf2",
   "metadata": {},
   "source": [
    "# 1. Set up the Huggingface Climate Policy Radar dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dd415c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "from functions import generate_embeddings_for_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4b6dc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d24f5ba666f409183819237aea1fb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2896b683c2ca47ac94bc0ada86144d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "\n",
    "ds = load_dataset(\"ClimatePolicyRadar/all-document-text-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b948c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.with_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "103174c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b443a98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>document_metadata.collection_summary</th>\n",
       "      <th>document_metadata.collection_title</th>\n",
       "      <th>document_metadata.corpus_type_name</th>\n",
       "      <th>document_metadata.corpus_import_id</th>\n",
       "      <th>document_metadata.category</th>\n",
       "      <th>document_metadata.description</th>\n",
       "      <th>document_metadata.document_title</th>\n",
       "      <th>document_metadata.family_import_id</th>\n",
       "      <th>document_metadata.family_slug</th>\n",
       "      <th>...</th>\n",
       "      <th>_html_data.has_valid_text</th>\n",
       "      <th>pipeline_metadata.parser_metadata</th>\n",
       "      <th>text_block.text_block_id</th>\n",
       "      <th>text_block.language</th>\n",
       "      <th>text_block.type</th>\n",
       "      <th>text_block.type_confidence</th>\n",
       "      <th>text_block.coords</th>\n",
       "      <th>text_block.page_number</th>\n",
       "      <th>text_block.text</th>\n",
       "      <th>text_block.index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCLW.document.i00000002.n0000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Laws and Policies</td>\n",
       "      <td>CCLW.corpus.i00000001.n0000</td>\n",
       "      <td>Executive</td>\n",
       "      <td>&lt;p&gt;&lt;span style=\"font-size: 10pt;font-family: A...</td>\n",
       "      <td>National Energy and Climate Plan 2019 Draft</td>\n",
       "      <td>CCLW.family.i00000001.n0000</td>\n",
       "      <td>national-energy-and-climate-plan_8a4f</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'azure_api_version': '2023-07-31', 'azure_mod...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>title</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[70.452, 123.7392], [524.1816, 123.7392], [52...</td>\n",
       "      <td>0</td>\n",
       "      <td>Draft of the National Energy and Climate Plan ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     document_id document_metadata.collection_summary  \\\n",
       "0  CCLW.document.i00000002.n0000                                 None   \n",
       "\n",
       "  document_metadata.collection_title document_metadata.corpus_type_name  \\\n",
       "0                               None                  Laws and Policies   \n",
       "\n",
       "  document_metadata.corpus_import_id document_metadata.category  \\\n",
       "0        CCLW.corpus.i00000001.n0000                  Executive   \n",
       "\n",
       "                       document_metadata.description  \\\n",
       "0  <p><span style=\"font-size: 10pt;font-family: A...   \n",
       "\n",
       "               document_metadata.document_title  \\\n",
       "0  National Energy and Climate Plan 2019 Draft    \n",
       "\n",
       "  document_metadata.family_import_id          document_metadata.family_slug  \\\n",
       "0        CCLW.family.i00000001.n0000  national-energy-and-climate-plan_8a4f   \n",
       "\n",
       "   ... _html_data.has_valid_text  \\\n",
       "0  ...                      None   \n",
       "\n",
       "                   pipeline_metadata.parser_metadata text_block.text_block_id  \\\n",
       "0  {'azure_api_version': '2023-07-31', 'azure_mod...                        0   \n",
       "\n",
       "  text_block.language text_block.type text_block.type_confidence  \\\n",
       "0                  en           title                        1.0   \n",
       "\n",
       "                                   text_block.coords text_block.page_number  \\\n",
       "0  [[70.452, 123.7392], [524.1816, 123.7392], [52...                      0   \n",
       "\n",
       "                                     text_block.text text_block.index  \n",
       "0  Draft of the National Energy and Climate Plan ...                0  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fbbbb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Draft of the National Energy and Climate Plan ...\n",
       "1                                            July 2021\n",
       "2                                 REPUBLIKA SHOIPERISE\n",
       "3                    MINISTRIA E TURIZMIT DHE MJEDISIT\n",
       "4            MINISTRIA E INFRASTRUKTURĒS DHE ENERGJISE\n",
       "5           german cooperation DEUTSCHE ZUSAMMENARBEIT\n",
       "6                                   Implemented by giz\n",
       "7    Deutsche Gesellschaft Für Internationale Zusam...\n",
       "8    Responsible for this document: Ministry of Inf...\n",
       "9    Purpose of this document: Submission to Energy...\n",
       "Name: text_block.text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:10][\"text_block.text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "215f5954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document_id', 'document_metadata.collection_summary', 'document_metadata.collection_title', 'document_metadata.corpus_type_name', 'document_metadata.corpus_import_id', 'document_metadata.category', 'document_metadata.description', 'document_metadata.document_title', 'document_metadata.family_import_id', 'document_metadata.family_slug', 'document_metadata.geographies', 'document_metadata.import_id', 'document_metadata.languages', 'document_metadata.metadata', 'document_metadata.family_title', 'document_metadata.publication_ts', 'document_metadata.slug', 'document_metadata.source', 'document_metadata.source_url', 'document_metadata.type', 'document_cdn_object', 'document_content_type', 'document_md5_sum', 'languages', 'document_metadata.translated', 'pdf_data_page_metadata.dimensions', '_html_data.detected_title', '_html_data.detected_date', '_html_data.has_valid_text', 'pipeline_metadata.parser_metadata', 'text_block.text_block_id', 'text_block.language', 'text_block.type', 'text_block.type_confidence', 'text_block.coords', 'text_block.page_number', 'text_block.text', 'text_block.index'],\n",
       "    num_rows: 34185184\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b0f84b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:10000][\"text_block.text\"].apply(len).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ce453",
   "metadata": {},
   "source": [
    "# Vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89ce6390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00dd9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_LOCAL_DIR = os.getenv('EMBEDDING_MODEL_LOCAL_DIR')\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9333973d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessiefung/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:732: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/jessiefung/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/jessiefung/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Download\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL, use_auth_token=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained(EMBEDDING_MODEL, use_auth_token=False)\n",
    "\n",
    "# Save it to a  local_models folder\n",
    "tokenizer.save_pretrained(EMBEDDING_MODEL_LOCAL_DIR)\n",
    "model.save_pretrained(EMBEDDING_MODEL_LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df923544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at local_model/climatebert/distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_LOCAL_DIR)\n",
    "model = AutoModel.from_pretrained(EMBEDDING_MODEL_LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3b691ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0add9e57c81a4351b3a102063a1bdc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/34185184 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/kv/l9vcg1gs7xb1tfhh37wfc4_w0000gn/T/ipykernel_35711/3915064436.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m contains_alb(example):\n\u001b[32m      2\u001b[39m     geos = example[\u001b[33m\"document_metadata.geographies\"\u001b[39m]\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m geos \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m \u001b[33m\"ALB\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m geos\n\u001b[32m      4\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m alb_chunks = chunks.filter(contains_alb)\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m             \u001b[33m\"columns\"\u001b[39m: self._format_columns,\n\u001b[32m    555\u001b[39m             \u001b[33m\"output_all_columns\"\u001b[39m: self._output_all_columns,\n\u001b[32m    556\u001b[39m         }\n\u001b[32m    557\u001b[39m         \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m         out: Union[\u001b[33m\"Dataset\"\u001b[39m, \u001b[33m\"DatasetDict\"\u001b[39m] = func(self, *args, **kwargs)\n\u001b[32m    559\u001b[39m         datasets: List[\u001b[33m\"Dataset\"\u001b[39m] = list(out.values()) \u001b[38;5;28;01mif\u001b[39;00m isinstance(out, dict) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    560\u001b[39m         \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n\u001b[32m    561\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;28;01min\u001b[39;00m datasets:\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/datasets/fingerprint.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m                         validate_fingerprint(kwargs[fingerprint_name])\n\u001b[32m    479\u001b[39m \n\u001b[32m    480\u001b[39m             \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[32m    481\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m             out = func(dataset, *args, **kwargs)\n\u001b[32m    483\u001b[39m \n\u001b[32m    484\u001b[39m             \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[32m    485\u001b[39m \n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3636\u001b[39m \n\u001b[32m   3637\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(self) == \u001b[32m0\u001b[39m:\n\u001b[32m   3638\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self\n\u001b[32m   3639\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m3640\u001b[39m         indices = self.map(\n\u001b[32m   3641\u001b[39m             function=partial(\n\u001b[32m   3642\u001b[39m                 get_indices_from_mask_function,\n\u001b[32m   3643\u001b[39m                 function,\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    589\u001b[39m             args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m    590\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    591\u001b[39m             self: \u001b[33m\"Dataset\"\u001b[39m = kwargs.pop(\u001b[33m\"self\"\u001b[39m)\n\u001b[32m    592\u001b[39m         \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m         out: Union[\u001b[33m\"Dataset\"\u001b[39m, \u001b[33m\"DatasetDict\"\u001b[39m] = func(self, *args, **kwargs)\n\u001b[32m    594\u001b[39m         datasets: List[\u001b[33m\"Dataset\"\u001b[39m] = list(out.values()) \u001b[38;5;28;01mif\u001b[39;00m isinstance(out, dict) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    595\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;28;01min\u001b[39;00m datasets:\n\u001b[32m    596\u001b[39m             \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m             \u001b[33m\"columns\"\u001b[39m: self._format_columns,\n\u001b[32m    555\u001b[39m             \u001b[33m\"output_all_columns\"\u001b[39m: self._output_all_columns,\n\u001b[32m    556\u001b[39m         }\n\u001b[32m    557\u001b[39m         \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m         out: Union[\u001b[33m\"Dataset\"\u001b[39m, \u001b[33m\"DatasetDict\"\u001b[39m] = func(self, *args, **kwargs)\n\u001b[32m    559\u001b[39m         datasets: List[\u001b[33m\"Dataset\"\u001b[39m] = list(out.values()) \u001b[38;5;28;01mif\u001b[39;00m isinstance(out, dict) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    560\u001b[39m         \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n\u001b[32m    561\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;28;01min\u001b[39;00m datasets:\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3101\u001b[39m                     unit=\u001b[33m\" examples\"\u001b[39m,\n\u001b[32m   3102\u001b[39m                     total=pbar_total,\n\u001b[32m   3103\u001b[39m                     desc=desc \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m\"Map\"\u001b[39m,\n\u001b[32m   3104\u001b[39m                 ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3105\u001b[39m                     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;28;01min\u001b[39;00m Dataset._map_single(**dataset_kwargs):\n\u001b[32m   3106\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[32m   3107\u001b[39m                             shards_done += \u001b[32m1\u001b[39m\n\u001b[32m   3108\u001b[39m                             logger.debug(f\"Finished processing shard number {rank} of {num_shards}.\")\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n\u001b[32m   3514\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3515\u001b[39m                         tmp_file.close()\n\u001b[32m   3516\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(tmp_file.name):\n\u001b[32m   3517\u001b[39m                             os.remove(tmp_file.name)\n\u001b[32m-> \u001b[39m\u001b[32m3518\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   3519\u001b[39m \n\u001b[32m   3520\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m rank, \u001b[38;5;28;01mFalse\u001b[39;00m, num_examples_progress_update\n\u001b[32m   3521\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;28;01mand\u001b[39;00m tmp_file \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[39m\n\u001b[32m   3357\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m with_indices:\n\u001b[32m   3358\u001b[39m                 additional_args += (effective_indices,)\n\u001b[32m   3359\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[32m   3360\u001b[39m                 additional_args += (rank,)\n\u001b[32m-> \u001b[39m\u001b[32m3361\u001b[39m             processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n\u001b[32m   3362\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m isinstance(processed_inputs, LazyDict):\n\u001b[32m   3363\u001b[39m                 processed_inputs = {\n\u001b[32m   3364\u001b[39m                     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;28;01min\u001b[39;00m processed_inputs.data.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m processed_inputs.keys_to_format\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(function, batched, with_indices, with_rank, input_columns, indices_mapping, *args, **fn_kwargs)\u001b[39m\n\u001b[32m   6249\u001b[39m             \u001b[38;5;66;03m# inputs only contains a batch of examples\u001b[39;00m\n\u001b[32m   6250\u001b[39m             batch: dict = inputs[\u001b[32m0\u001b[39m]\n\u001b[32m   6251\u001b[39m             num_examples = len(batch[next(iter(batch.keys()))])\n\u001b[32m   6252\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;28;01min\u001b[39;00m range(num_examples):\n\u001b[32m-> \u001b[39m\u001b[32m6253\u001b[39m                 example = {key: batch[key][i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m batch}\n\u001b[32m   6254\u001b[39m                 additional_args = ()\n\u001b[32m   6255\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m with_indices:\n\u001b[32m   6256\u001b[39m                     additional_args += (indices[i],)\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4074\u001b[39m                 self.columns.is_unique\n\u001b[32m   4075\u001b[39m                 \u001b[38;5;28;01mand\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m self.columns\n\u001b[32m   4076\u001b[39m                 \u001b[38;5;28;01mor\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m self.columns.drop_duplicates(keep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   4077\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m4078\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m self._get_item_cache(key)\n\u001b[32m   4079\u001b[39m \n\u001b[32m   4080\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m is_mi \u001b[38;5;28;01mand\u001b[39;00m self.columns.is_unique \u001b[38;5;28;01mand\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m self.columns:\n\u001b[32m   4081\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m self._getitem_multilevel(key)\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   4635\u001b[39m             \u001b[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[32m   4636\u001b[39m             \u001b[38;5;66;03m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[32m   4637\u001b[39m \n\u001b[32m   4638\u001b[39m             loc = self.columns.get_loc(item)\n\u001b[32m-> \u001b[39m\u001b[32m4639\u001b[39m             res = self._ixs(loc, axis=\u001b[32m1\u001b[39m)\n\u001b[32m   4640\u001b[39m \n\u001b[32m   4641\u001b[39m             cache[item] = res\n\u001b[32m   4642\u001b[39m \n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, i, axis)\u001b[39m\n\u001b[32m   4007\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4008\u001b[39m             label = self.columns[i]\n\u001b[32m   4009\u001b[39m \n\u001b[32m   4010\u001b[39m             col_mgr = self._mgr.iget(i)\n\u001b[32m-> \u001b[39m\u001b[32m4011\u001b[39m             result = self._box_col_values(col_mgr, i)\n\u001b[32m   4012\u001b[39m \n\u001b[32m   4013\u001b[39m             \u001b[38;5;66;03m# this is a cached value, mark it so\u001b[39;00m\n\u001b[32m   4014\u001b[39m             result._set_as_cached(label, self)\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, values, loc)\u001b[39m\n\u001b[32m   4613\u001b[39m         \u001b[38;5;66;03m#  we attach the Timestamp object as the name.\u001b[39;00m\n\u001b[32m   4614\u001b[39m         name = self.columns[loc]\n\u001b[32m   4615\u001b[39m         \u001b[38;5;66;03m# We get index=self.index bc values is a SingleDataManager\u001b[39;00m\n\u001b[32m   4616\u001b[39m         obj = self._constructor_sliced_from_mgr(values, axes=values.axes)\n\u001b[32m-> \u001b[39m\u001b[32m4617\u001b[39m         obj._name = name\n\u001b[32m   4618\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m obj.__finalize__(self)\n",
      "\u001b[32m~/Desktop/DS205/group-6-final-project/.venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6310\u001b[39m \n\u001b[32m   6311\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   6312\u001b[39m             object.__getattribute__(self, name)\n\u001b[32m   6313\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m object.__setattr__(self, name, value)\n\u001b[32m-> \u001b[39m\u001b[32m6314\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m AttributeError:\n\u001b[32m   6315\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   6316\u001b[39m \n\u001b[32m   6317\u001b[39m         \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def contains_alb(example):\n",
    "    geos = example[\"document_metadata.geographies\"]\n",
    "    return geos is not None and \"ALB\" in geos\n",
    "\n",
    "alb_chunks = chunks.filter(contains_alb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92703641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9f43e428cf4099a63d06db77f2f2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13df9e51743846abb9fb96770c7cad50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caabfc0b6eef434cb7f3f43d582f7ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Ensure the \"data\" directory exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Process embeddings in batches of 1000\n",
    "batch_size = 1000\n",
    "all_batches = (len(chunks) + batch_size - 1) // batch_size  # Calculate the number of batches\n",
    "num_batches = 2\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(chunks))\n",
    "    \n",
    "    # Generate embeddings for the current batch\n",
    "    batch_embeddings = chunks[start_idx:end_idx][\"text_block.text\"].progress_apply(\n",
    "        lambda text: generate_embeddings_for_text(text, model, tokenizer)\n",
    "    )\n",
    "    \n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "# Create a DataFrame for all embeddings\n",
    "embeddings_df = pd.DataFrame({\n",
    "    \"document_id\": chunks[:num_batches*1000][\"document_id\"],\n",
    "    \"embeddings\": all_embeddings\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a single CSV file\n",
    "embeddings_df.to_csv(\"data/embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0356f563",
   "metadata": {},
   "source": [
    "Use DPR for question answering, using chunks[\"text_block.text\"] as context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
