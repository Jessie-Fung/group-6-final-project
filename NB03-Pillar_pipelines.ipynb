{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a19313",
   "metadata": {},
   "source": [
    "# Setup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7623a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import BaseOutputParser\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langdetect import detect\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a36b8",
   "metadata": {},
   "source": [
    "# NEED TO ADD CITATIONS!!!!!!!!!1\n",
    "\n",
    "use https://python.langchain.com/docs/how_to/qa_citations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2087b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-5201472be1a3433cbcf14a68c96e3330\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"Hello! How can I assist you today?\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1748194898,\n",
      "  \"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 10,\n",
      "    \"prompt_tokens\": 12,\n",
      "    \"total_tokens\": 22,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Hello!\"\"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "print(completion.to_json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "multilingual_llm = ChatOpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "    model=\"Qwen/Qwen3-32B\",\n",
    "    temperature=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b4a999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "# Create a simple prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"assistant\", \"How can I assist you today?\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create a simple chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Test it\n",
    "response = chain.invoke({\"question\": \"Hello!\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca3ec3",
   "metadata": {},
   "source": [
    "# CP 1. Climate legislation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f645c4",
   "metadata": {},
   "source": [
    "##### Indicator CP 1.a Does the country have a framework climate law or equivalent?\n",
    "\n",
    "Individualy test the following (A country is assessed as ‘Yes’ if it has a framework climate law that fulfils all of\n",
    "the following):\n",
    "\n",
    "1. It sets a strategic direction for decarbonisation (i.e. it must include a clear\n",
    "statement to meet the goals of the Paris Agreement or a national long-term\n",
    "decarbonisation target)\n",
    "\n",
    "2. It is enshrined in law (i.e. it must be legislative rather than executive, except in\n",
    "particular political systems)\n",
    "\n",
    "3. It sets out at least one of the following obligations: meeting a national target;\n",
    "developing, revising, implementing or complying with domestic plans,\n",
    "strategies or policies; developing policy instruments such as regulation, taxation\n",
    "or public spending in support of climate change goals\n",
    "\n",
    "4. Also check this In exceptional cases, the combination of a broad environmental law and a clearly\n",
    "linked executive climate strategy may be sufficient to meet these criteria\n",
    "\n",
    "**Then have other model check if 1,2,3 are satisfied or 4 is satisfied -> answer yes**\n",
    "\n",
    "**Then have another model format output into markdown using (this)[https://python.langchain.com/docs/tutorials/extraction/]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a0046",
   "metadata": {},
   "source": [
    "### 1.1 Chain of thought Implementation\n",
    "#### 1.1.1 Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa2bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define individual criterion evaluation prompts\n",
    "criterion_1_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation. \n",
    "    Your task is to evaluate whether a climate law sets a strategic direction for decarbonisation.\n",
    "    \n",
    "    A law meets this criterion if it includes a clear statement to meet the goals of the Paris Agreement \n",
    "    OR a national long-term decarbonisation target.\n",
    "    \n",
    "    Respond with only 'YES' or 'NO' followed by a brief explanation.\"\"\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nDoes this law set a strategic direction for decarbonisation?\")\n",
    "])\n",
    "\n",
    "criterion_2_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation.\n",
    "    Your task is to evaluate whether a climate law is enshrined in law.\n",
    "    \n",
    "    A law meets this criterion if it is legislative rather than executive \n",
    "    (except in particular political systems where executive action has legal force).\n",
    "    \n",
    "    Respond with only 'YES' or 'NO' followed by a brief explanation.\"\"\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nIs this law enshrined in law (legislative rather than executive)?\")\n",
    "])\n",
    "\n",
    "criterion_3_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation.\n",
    "    Your task is to evaluate whether a climate law sets out specific obligations.\n",
    "    \n",
    "    A law meets this criterion if it sets out at least one of the following:\n",
    "    - Meeting a national target\n",
    "    - Developing, revising, implementing or complying with domestic plans, strategies or policies\n",
    "    - Developing policy instruments such as regulation, taxation or public spending in support of climate goals\n",
    "    \n",
    "    Respond with only 'YES' or 'NO' followed by a brief explanation.\"\"\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nDoes this law set out the required obligations?\")\n",
    "])\n",
    "\n",
    "criterion_4_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation.\n",
    "    Your task is to evaluate the exceptional case criterion.\n",
    "    \n",
    "    This criterion is met if there is a combination of:\n",
    "    - A broad environmental law AND\n",
    "    - A clearly linked executive climate strategy\n",
    "    \n",
    "    This combination may be sufficient to meet the framework criteria in exceptional cases.\n",
    "    \n",
    "    Respond with only 'YES' or 'NO' followed by a brief explanation.\"\"\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nDoes this represent a valid exceptional case (broad environmental law + linked executive strategy)?\")\n",
    "])\n",
    "\n",
    "# Final assessment prompt\n",
    "final_assessment_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst making a final assessment of climate legislation.\n",
    "    \n",
    "    A country is assessed as 'YES' for having framework climate law if:\n",
    "    - Criteria 1, 2, AND 3 are all satisfied, OR\n",
    "    - Criterion 4 is satisfied (exceptional case)\n",
    "    \n",
    "    Based on the individual assessments, provide a final 'YES' or 'NO' answer with reasoning.\"\"\"),\n",
    "    (\"human\", \"\"\"Individual criterion assessments:\n",
    "    Criterion 1 (Strategic direction): {criterion_1_result}\n",
    "    Criterion 2 (Enshrined in law): {criterion_2_result}\n",
    "    Criterion 3 (Obligations): {criterion_3_result}\n",
    "    Criterion 4 (Exceptional case): {criterion_4_result}\n",
    "    \n",
    "    What is the final assessment?\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb78ff",
   "metadata": {},
   "source": [
    "#### 1.1.2 Retrieve Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9efa30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "977e5701",
   "metadata": {},
   "source": [
    "#### 1.1.4 Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dcdb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Assessment:\n",
      "Based on the individual assessments, I conclude that the country has a framework climate law: **YES**\n",
      "\n",
      "The reason is that all three main criteria (1, 2, and 3) are satisfied. The law provides a clear strategic direction, is enshrined in law, and imposes obligations on the country to develop national climate plans and carbon pricing mechanisms. Since Criterion 4 is not satisfied, it is not an exceptional case, and we rely on the satisfaction of the first three criteria.\n"
     ]
    }
   ],
   "source": [
    "# Standard chains\n",
    "criterion_1_chain = criterion_1_prompt | llm\n",
    "criterion_2_chain = criterion_2_prompt | llm\n",
    "criterion_3_chain = criterion_3_prompt | llm\n",
    "criterion_4_chain = criterion_4_prompt | llm\n",
    "\n",
    "# Multilingual chains\n",
    "criterion_1_chain_noneng = criterion_1_prompt | multilingual_llm\n",
    "criterion_2_chain_noneng = criterion_2_prompt | multilingual_llm\n",
    "criterion_3_chain_noneng = criterion_3_prompt | multilingual_llm\n",
    "criterion_4_chain_noneng = criterion_4_prompt | multilingual_llm\n",
    "\n",
    "\n",
    "# Create an enhanced evaluation function that routes based on language\n",
    "def evaluate_all_criteria_multilingual(inputs):\n",
    "    \"\"\"Evaluate all four criteria using appropriate LLM based on language detection\"\"\"\n",
    "    \n",
    "    # Detect languages for each context\n",
    "    lang_1 = detect_language(inputs[\"context_1\"])\n",
    "    lang_2 = detect_language(inputs[\"context_2\"])\n",
    "    lang_3 = detect_language(inputs[\"context_3\"])\n",
    "    lang_4 = detect_language(inputs[\"context_4\"])\n",
    "    \n",
    "    # Helper function to choose appropriate chain\n",
    "    def get_chain_for_language(language, standard_chain, multilingual_chain):\n",
    "        return multilingual_chain if language != 'en' else standard_chain\n",
    "    \n",
    "    # Route to appropriate chains based on language\n",
    "    chain_1 = get_chain_for_language(lang_1, criterion_1_chain, criterion_1_chain_noneng)\n",
    "    chain_2 = get_chain_for_language(lang_2, criterion_2_chain, criterion_2_chain_noneng)\n",
    "    chain_3 = get_chain_for_language(lang_3, criterion_3_chain, criterion_3_chain_noneng)\n",
    "    chain_4 = get_chain_for_language(lang_4, criterion_4_chain, criterion_4_chain_noneng)\n",
    "    \n",
    "    # Execute evaluations\n",
    "    criterion_1_result = chain_1.invoke({\"context\": inputs[\"context_1\"]})\n",
    "    criterion_2_result = chain_2.invoke({\"context\": inputs[\"context_2\"]})\n",
    "    criterion_3_result = chain_3.invoke({\"context\": inputs[\"context_3\"]})\n",
    "    criterion_4_result = chain_4.invoke({\"context\": inputs[\"context_4\"]})\n",
    "    \n",
    "    return {\n",
    "        \"criterion_1_result\": criterion_1_result.content,\n",
    "        \"criterion_2_result\": criterion_2_result.content,\n",
    "        \"criterion_3_result\": criterion_3_result.content,\n",
    "        \"criterion_4_result\": criterion_4_result.content,\n",
    "        \"languages_detected\": {\n",
    "            \"context_1\": lang_1,\n",
    "            \"context_2\": lang_2,\n",
    "            \"context_3\": lang_3,\n",
    "            \"context_4\": lang_4\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Update the complete assessment chain\n",
    "complete_assessment_chain_multilingual = (\n",
    "    RunnableLambda(evaluate_all_criteria_multilingual)\n",
    "    | final_assessment_prompt \n",
    "    | llm  # Final assessment can stay in English\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abdbc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline\n",
    "result = complete_assessment_chain_multilingual.invoke(test_contexts)\n",
    "#some function that takes indicator, retrieves context and then runs complete_assessment_chain fucntion\n",
    "print(\"Final Assessment:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626a015",
   "metadata": {},
   "source": [
    "### 1.1.5 Add citations\n",
    "\n",
    "Try generation post processing langchain approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "# Data models for your existing chunk metadata\n",
    "class ChunkMetadata(BaseModel):\n",
    "    \"\"\"Metadata for a text chunk used in generation\"\"\"\n",
    "    chunk_id: int = Field(..., description=\"Unique identifier for this chunk\")\n",
    "    document_name: str = Field(..., description=\"Name of the source document\")\n",
    "    page_number: int = Field(..., description=\"Page number in the document\")\n",
    "    paragraph_number: int = Field(..., description=\"Paragraph number on the page\")\n",
    "    chunk_text: str = Field(..., description=\"The actual text content of the chunk\")\n",
    "\n",
    "# Citation model for sentence-level citations\n",
    "class SentenceCitation(BaseModel):\n",
    "    \"\"\"Citation information for a single sentence\"\"\"\n",
    "    document_name: str = Field(..., description=\"Name of the source document\")\n",
    "    page_number: int = Field(..., description=\"Page number in the document\") \n",
    "    paragraph_number: int = Field(..., description=\"Paragraph number on the page\")\n",
    "    \n",
    "    def format_citation(self) -> str:\n",
    "        \"\"\"Format citation as inline text\"\"\"\n",
    "        return f\"({self.document_name}, p. {self.page_number}, ¶{self.paragraph_number})\"\n",
    "\n",
    "# Model for a sentence with its citation\n",
    "class CitedSentence(BaseModel):\n",
    "    \"\"\"A sentence paired with its supporting citation\"\"\"\n",
    "    sentence_text: str = Field(..., description=\"The complete sentence text\")\n",
    "    citation: SentenceCitation = Field(..., description=\"Citation supporting this sentence\")\n",
    "    \n",
    "    def format_cited_sentence(self) -> str:\n",
    "        \"\"\"Format sentence with inline citation\"\"\"\n",
    "        return f\"{self.sentence_text} {self.citation.format_citation()}\"\n",
    "\n",
    "# Main structured output model\n",
    "class AnnotatedText(BaseModel):\n",
    "    \"\"\"Complete text broken into sentences with citations\"\"\"\n",
    "    cited_sentences: List[CitedSentence] = Field(\n",
    "        ..., \n",
    "        description=\"List of sentences from the generated text, each with its supporting citation\"\n",
    "    )\n",
    "    \n",
    "    def format_full_text(self) -> str:\n",
    "        \"\"\"Reconstruct the full text with citations after each sentence\"\"\"\n",
    "        return \" \".join([cs.format_cited_sentence() for cs in self.cited_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf40622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to format chunks for LLM context\n",
    "def format_chunks_with_metadata(chunks: List[ChunkMetadata]) -> str:\n",
    "    \"\"\"Format chunks with IDs and metadata for LLM processing\"\"\"\n",
    "    formatted_chunks = []\n",
    "    for chunk in chunks:\n",
    "        chunk_info = f\"\"\"\n",
    "Chunk ID: {chunk.chunk_id}\n",
    "Document: {chunk.document_name}\n",
    "Page: {chunk.page_number}\n",
    "Paragraph: {chunk.paragraph_number}\n",
    "Content: {chunk.chunk_text}\n",
    "\"\"\"\n",
    "        formatted_chunks.append(chunk_info)\n",
    "    \n",
    "    return \"\\n\" + \"=\"*50 + \"\\n\".join(formatted_chunks)\n",
    "\n",
    "# LangChain integration function\n",
    "def create_citation_prompt(generated_text: str, chunks: List[ChunkMetadata]) -> str:\n",
    "    \"\"\"Create the prompt for citation annotation\"\"\"\n",
    "    formatted_chunks = format_chunks_with_metadata(chunks)\n",
    "    \n",
    "    system_prompt = f\"\"\"You are tasked with adding citations to generated text. \n",
    "\n",
    "AVAILABLE SOURCE CHUNKS:\n",
    "{formatted_chunks}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Break the generated text into individual sentences\n",
    "2. For each sentence, identify which chunk(s) from the available sources best support that sentence\n",
    "3. Create a citation using the document name, page number, and paragraph number\n",
    "4. Return the structured output with each sentence paired with its citation\n",
    "\n",
    "GENERATED TEXT TO ANNOTATE:\n",
    "{generated_text}\n",
    "\n",
    "Please return each sentence with its appropriate citation based on the source chunks provided.\"\"\"\n",
    "    \n",
    "    return system_prompt\n",
    "\n",
    "# Example usage structure for LangChain integration\n",
    "def annotate_with_citations(generated_text: str, chunks: List[ChunkMetadata], llm):\n",
    "    \"\"\"\n",
    "    Main function to add citations to generated text using LangChain\n",
    "    \n",
    "    Args:\n",
    "        generated_text: The text generated by your LLM\n",
    "        chunks: List of chunks with metadata that were used for generation\n",
    "        llm: Your LangChain LLM instance with structured output\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create structured LLM\n",
    "    structured_llm = llm.with_structured_output(AnnotatedText)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt_text = create_citation_prompt(generated_text, chunks)\n",
    "    \n",
    "    # Get structured response\n",
    "    response = structured_llm.invoke([(\"human\", prompt_text)])\n",
    "    \n",
    "    # Return formatted text with citations\n",
    "    return response.format_full_text()\n",
    "\n",
    "# Example of how your data might look\n",
    "example_chunks = [\n",
    "    ChunkMetadata(\n",
    "        chunk_id=1,\n",
    "        document_name=\"Climate Report 2024\",\n",
    "        page_number=15,\n",
    "        paragraph_number=3,\n",
    "        chunk_text=\"Global temperatures have risen by 1.2°C since pre-industrial times.\"\n",
    "    ),\n",
    "    ChunkMetadata(\n",
    "        chunk_id=2,\n",
    "        document_name=\"Climate Report 2024\", \n",
    "        page_number=23,\n",
    "        paragraph_number=1,\n",
    "        chunk_text=\"Sea levels are projected to rise between 0.5 and 2.5 meters by 2100.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "example_generated_text = \"Climate change has caused significant global warming in recent decades. Ocean levels are expected to increase dramatically over the next century.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18e75b",
   "metadata": {},
   "source": [
    "#### 1.2 Alternative Large context window Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45807249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load larger context LLM\n",
    "large_context_llm = ChatOpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    temperature=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5057e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set context to whole entire corpus of relevant documetns according to slyvan sheet\n",
    "\n",
    "all_documents_context ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea79a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single comprehensive climate legislation assessment prompt\n",
    "comprehensive_assessment_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation assessment. \n",
    "\n",
    "Your task is to evaluate whether a country has a framework climate law based on specific criteria and provide a structured markdown assessment.\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "A country is assessed as 'YES' if it has a framework climate law that fulfils ALL of criteria 1, 2, AND 3, OR criterion 4:\n",
    "\n",
    "1. STRATEGIC DIRECTION: Sets a strategic direction for decarbonisation (must include a clear statement to meet the goals of the Paris Agreement OR a national long-term decarbonisation target)\n",
    "\n",
    "2. ENSHRINED IN LAW: Is enshrined in law (must be legislative rather than executive, except in particular political systems)\n",
    "\n",
    "3. OBLIGATIONS: Sets out at least one of the following obligations:\n",
    "   - Meeting a national target\n",
    "   - Developing, revising, implementing or complying with domestic plans, strategies or policies\n",
    "   - Developing policy instruments such as regulation, taxation or public spending in support of climate goals\n",
    "\n",
    "4. EXCEPTIONAL CASE: The combination of a broad environmental law AND a clearly linked executive climate strategy may be sufficient to meet these criteria\n",
    "\n",
    "ASSESSMENT LOGIC:\n",
    "- If criteria 1, 2, AND 3 are all satisfied → YES\n",
    "- If criterion 4 is satisfied → YES\n",
    "- Otherwise → NO\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Provide your assessment in the following markdown format:\n",
    "\n",
    "```markdown\n",
    "# Climate Legislation Assessment: CP 1.a Framework Climate Law\n",
    "\n",
    "## Individual Criterion Evaluation\n",
    "\n",
    "### Criterion 1: Strategic Direction for Decarbonisation\n",
    "**Result:** [YES/NO]\n",
    "**Reasoning:** [Brief explanation of whether the law includes clear Paris Agreement goals or long-term decarbonisation targets]\n",
    "\n",
    "### Criterion 2: Enshrined in Law\n",
    "**Result:** [YES/NO]\n",
    "**Reasoning:** [Brief explanation of whether this is legislative rather than executive]\n",
    "\n",
    "### Criterion 3: Sets Out Obligations\n",
    "**Result:** [YES/NO]\n",
    "**Reasoning:** [Brief explanation of which obligations are present, if any]\n",
    "\n",
    "### Criterion 4: Exceptional Case\n",
    "**Result:** [YES/NO]\n",
    "**Reasoning:** [Brief explanation of whether broad environmental law + executive strategy combination exists]\n",
    "\n",
    "## Final Assessment\n",
    "\n",
    "**Overall Result:** [YES/NO]\n",
    "\n",
    "**Logic Applied:** [Explain whether criteria 1+2+3 are satisfied OR criterion 4 is satisfied]\n",
    "\n",
    "**Conclusion:** [Brief summary of why the country does/does not have a framework climate law]\n",
    "```\"\"\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nPlease evaluate whether this country has a framework climate law based on the provided context.\")\n",
    "])\n",
    "\n",
    "# Create the single-model chain\n",
    "single_model_assessment_chain = comprehensive_assessment_prompt | large_context_llm\n",
    "\n",
    "\n",
    "# Run the assessment\n",
    "result = single_model_assessment_chain.invoke({\"context\": all_documents_context})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49af5d",
   "metadata": {},
   "source": [
    "# CP1b - Does the country’s framework climate law specify key accountability elements?\n",
    "\n",
    "\n",
    "A country is assessed as ‘Yes’ if its framework climate law contains all three of the following accountability elements: \n",
    "1. Specification of who is accountable to whom for at least one stated obligation (e.g. accountability of executive to parliament, or private parties to executive authorities) \n",
    "2. Specification of how compliance is assessed for at least one stated obligation (e.g. transparency mechanisms in the form of monitoring, reporting and verification, parliamentary oversight, expert assessments, court proceedings) \n",
    "3. Specification of what happens in the case of non-compliance for at least one stated obligation (e.g. parliamentary intervention, judicial orders, financial penalties). \n",
    "\n",
    "For each of the accountability processes outlined these are examples of the types of relationships that usually qualify for the sub-questions. Each sub-question must be satisfied by at least one of the types of accountability.\n",
    "1. Who is accountable to whom?\n",
    "Examples of key relationships include:\n",
    "a.\texecutive to parliament \n",
    "b.\texecutive to executive and/or administrative agencies \n",
    "c.\tnational to sub-national and sub-national to national\n",
    "d.\texecutive to judiciary\n",
    "e.\texecutive to expert bodies\n",
    "f.\texecutive to citizens \n",
    "g.\tprivate parties to citizens \n",
    "h.\tprivate parties to executive authorities.\n",
    "2. How is compliance assessed?\n",
    "Some of the most common ways of assessing compliance are:\n",
    "a.\ttransparency mechanisms in the form of monitoring, reporting and verification (MRV). Although this type of mechanism does not specify a decision-maker responsible for assessing whether an obligation has been met, this type of process is crucial for ensuring that there is political accountability for the implementation of climate law\n",
    "b.\tparliamentary oversight, which can also be used to assess the effectiveness of the legislation in achieving the stated aims\n",
    "c.\tExpert assessment\n",
    "d.\tCourt proceedings\n",
    "e.\tAlternative Dispute Resolution.\n",
    "3. What happens in the case of non-compliance?\n",
    "It is relatively rare for countries to specify what happens in the case of non-compliance. Relevant practices identified in the laws can be grouped under certain categories. Some examples are:\n",
    "a.\tParliamentary intervention\n",
    "b.\tGovernmental or ministerial intervention\n",
    "c.\tJudicial orders\n",
    "d.\tOrders and fines by regulators\n",
    "e.\tCourt imposed financial penalties\n",
    "f.\tRegulatory financial penalties\n",
    "g.\tMulti-stakeholder agreements.\n",
    "Note that if a country has multiple laws assessed under the previous indicator, all are considered under this indicator on whether they together contain the three accountability elements above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc802d1",
   "metadata": {},
   "source": [
    "### 2.1 Retreive Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a0376",
   "metadata": {},
   "source": [
    "### 2.2 Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edca86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define individual criterion evaluation prompts\n",
    "cp1b_criterion_1_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation. \n",
    "    Your task is to evaluate whether a country's framework climate **Specification of who is accountable to whom for at least one stated obligation \n",
    "    (e.g. accountability of executive to parliament, or private parties to executive authorities)**\n",
    "    \n",
    "    These are examples of the types of relationships that usually qualify as having specification of who is accountable to whom:\n",
    "        a.\texecutive to parliament \n",
    "        b.\texecutive to executive and/or administrative agencies \n",
    "        c.\tnational to sub-national and sub-national to national\n",
    "        d.\texecutive to judiciary\n",
    "        e.\texecutive to expert bodies\n",
    "        f.\texecutive to citizens \n",
    "        g.\tprivate parties to citizens \n",
    "        h.\tprivate parties to executive authorities.\n",
    "    \n",
    "    Respond with only 'YES' or 'NO' followed by a brief explanation.\"\"\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nDoes this country's framework climate law specify who is accountable to whom for at least one stated obligation?\")\n",
    "])\n",
    "\n",
    "cp1b_criterion_2_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation.\n",
    "    Your task is to evaluate whether a country's framework climate law contains the following accountability element:\n",
    "    **specification of how compliance is assessed for at least one stated obligation \n",
    "    (e.g. transparency mechanisms in the form of monitoring, reporting and verification, parliamentary oversight, expert assessments, court proceedings)**\n",
    "        \n",
    "    Some of the most common ways of assessing compliance are:\n",
    "    a.\ttransparency mechanisms in the form of monitoring, reporting and verification (MRV). Although this type of mechanism does not specify a decision-maker responsible for assessing whether an obligation has been met, this type of process is crucial for ensuring that there is political accountability for the implementation of climate law\n",
    "    b.\tparliamentary oversight, which can also be used to assess the effectiveness of the legislation in achieving the stated aims\n",
    "    c.\tExpert assessment\n",
    "    d.\tCourt proceedings\n",
    "    e.\tAlternative Dispute Resolution.\n",
    "    \n",
    "    Respond with only 'YES' or 'NO' followed by a brief explanation.\"\"\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nIs this country's framework climate law specification of how compliance is assessed for at least one stated obligation?\")\n",
    "])\n",
    "\n",
    "cp1b_criterion_3_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation.\n",
    "    Your task is to evaluate whether a country's framework climate law contains the following accountability element:\n",
    "        **Specification of what happens in the case of non-compliance for at least one stated obligation (e.g. parliamentary intervention, judicial orders, financial penalties).**\n",
    "        \n",
    "    It is relatively rare for countries to specify what happens in the case of non-compliance. Relevant practices identified in the laws can be grouped under certain categories. Some examples are:\n",
    "    a.\tParliamentary intervention\n",
    "    b.\tGovernmental or ministerial intervention\n",
    "    c.\tJudicial orders\n",
    "    d.\tOrders and fines by regulators\n",
    "    e.\tCourt imposed financial penalties\n",
    "    f.\tRegulatory financial penalties\n",
    "    g.\tMulti-stakeholder agreements.\n",
    "    Note that if a country has multiple laws assessed under the previous indicator, all are considered under this indicator on whether they together contain the three accountability elements above.\n",
    "    \n",
    "    Respond with only 'YES' or 'NO' followed by a brief explanation.\"\"\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nDoes this country's framework climate law specify what happens in the case of non-compliance for at least one stated obligation?\")\n",
    "])\n",
    "\n",
    "# Final assessment prompt\n",
    "cp1b_final_assessment_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst making a final assessment of climate legislation.\n",
    "    \n",
    "    A country is assessed as 'YES' if the country's framework climate law specifies ALL THREE of the following key accountability elements:\n",
    "        Criterion 1: Specification of who is accountable to whom for at least one stated obligation\n",
    "        Criterion 2: Specification of how compliance is assessed for at least one stated obligation\n",
    "        Criterion 3: Specification of what happens in the case of non-compliance for at least one stated obligation\n",
    "\n",
    "    If YES to all criteria, provide a final 'YES' or 'NO' answer with reasoning.\"\"\"),\n",
    "    (\"human\", \"\"\"Individual criterion assessments:\n",
    "    Criterion 1 (who is accountable to whom): {criterion_1_result}\n",
    "    Criterion 2 (compliance is assessed): {criterion_2_result}\n",
    "    Criterion 3 (case of non-compliance): {criterion_3_result}\n",
    "    \n",
    "    What is the final assessment?\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c931ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add implementation here if cp1a code works"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group_rag.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
