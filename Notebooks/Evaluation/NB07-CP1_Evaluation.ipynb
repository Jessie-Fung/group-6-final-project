{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbaedbd",
   "metadata": {},
   "source": [
    "# CP1 Evaluation Performance\n",
    "\n",
    "Lets see how "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the absolute path of the project root directory\n",
    "notebook_dir = Path(os.getcwd())  \n",
    "project_root = notebook_dir.parent.parent  # Go up TWO levels instead of one\n",
    "\n",
    "# Add project root to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Added {project_root} to sys.path\")\n",
    "\n",
    "from scripts.climate_policy_pipelines.cp1.pipeline import run_cp1a_assessment\n",
    "from scripts.climate_policy_pipelines.cp1.pipeline import run_cp1a_assessment_large_context\n",
    "from scripts.climate_policy_pipelines.cp1.pipeline import run_cp1b_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88713fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ASCOR data\n",
    "\n",
    "ascor_ground_truth = read_xlsx(\"Notebooks\\\\Evaluation\\\\ASCOR_assessments_results.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4551d",
   "metadata": {},
   "source": [
    "# Run Assement \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b81bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ascor_ground_truth[\"Country\"].unique().tolist()[:10]\n",
    "\n",
    "results_data = []\n",
    "\n",
    "for country in countries:\n",
    "    # Get ASCOR ground truth for this country\n",
    "    ascor_result = ascor_ground_truth[ascor_ground_truth[\"Country\"] == country]\n",
    "    \n",
    "    # Run CP1A assessment and capture result\n",
    "    cp1a_rag = run_cp1a_assessment(country, detailed=False, print_results=False)\n",
    "    cp1a_large_context_rag = run_cp1a_assessment_large_context(country, detailed=False, print_results=False)\n",
    "    cp1b_rag = run_cp1b_assessment(country, detailed=False, print_results=False)  # Uncomment if needed\n",
    "    \n",
    "    # Store results (adjust column names based on your ASCOR data structure)\n",
    "    results_data.append({\n",
    "        'Country': country,\n",
    "        'ASCOR_True': ascor_result.iloc[0]['Assessment_Column'],  # Replace with actual column name\n",
    "        'CP1A_Assessment': cp1a_rag, # Assuming this returns the assessment score/result\n",
    "        'CP1A_Large_Context_Assessment': cp1a_large_context_rag,  # Assuming this returns the assessment score/result\n",
    "        'CP1B_Assessment': cp1b_rag  # Uncomment if needed\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b64d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Calculate accuracy for each assessment method\n",
    "cp1a_accuracy = (comparison_df['ASCOR_True'] == comparison_df['CP1A_Assessment']).mean()\n",
    "cp1a_large_accuracy = (comparison_df['ASCOR_True'] == comparison_df['CP1A_Large_Context_Assessment']).mean()\n",
    "cp1b_accuracy = (comparison_df['ASCOR_True'] == comparison_df['CP1B_Assessment']).mean()\n",
    "\n",
    "print(f\"CP1A Accuracy: {cp1a_accuracy:.2%}\")\n",
    "print(f\"CP1A Large Context Accuracy: {cp1a_large_accuracy:.2%}\")\n",
    "print(f\"CP1B Accuracy: {cp1b_accuracy:.2%}\")\n",
    "\n",
    "# Create visualization\n",
    "methods = ['CP1A', 'CP1A Large Context', 'CP1B']\n",
    "accuracies = [cp1a_accuracy, cp1a_large_accuracy, cp1b_accuracy]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(methods, accuracies, color=['skyblue', 'lightgreen', 'coral'])\n",
    "plt.title('Assessment Method Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.1%}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display comparison DataFrame\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
