{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a100c321",
   "metadata": {},
   "source": [
    "This tool is built to create reports on any topic requested using the climate tracker database\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import BaseOutputParser\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c1a76",
   "metadata": {},
   "source": [
    "**play around whith which models work best for different parts of the pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f6339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "super_basic_model = ChatOpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "standard_model = ChatOpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "larger_context_model = ChatOpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    temperature=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac2c73",
   "metadata": {},
   "source": [
    "#### 1.1 Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b82d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the template generating LLM\n",
    "template_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation. \n",
    "    Your task is to create a template for the structure of a 1 page report on {topic}. \n",
    "     \n",
    "     Here are some example templates to base your response on:\n",
    "        ##METADATA \n",
    "\n",
    "        *Country: \n",
    "\n",
    "        ##REPORT \n",
    "\n",
    "        Q1: What are the relevant background information for indicator X, Y or Z? \n",
    "        … \n",
    "        Question N (CHALLENGING): How has the legislation of this country changed in the past 50 years? \n",
    "        \n",
    "     \n",
    "        ##TOPIC: Just transition \n",
    "\n",
    "        *Countries with relevant information: … \n",
    "\n",
    "        ##REPORT \n",
    "\n",
    "        Q1: Which countries have made plans for a just energetic transition? \n",
    "\n",
    "        A: Country A [citation], B [citation] and C [citation] have a similar law. They all promise X, Y and Z. On the other hand, the following countries … differ because … \n",
    "        A: … \n",
    "        \n",
    "        A law meets this criterion if it includes a clear statement to meet the goals of the Paris Agreement \n",
    "        OR a national long-term decarbonisation target.\n",
    "        \n",
    "    Respond with only the template of the {topic} report and nothing else.\"\"\"),\n",
    "    (\"human\", \"Context: {topic}\")\n",
    "])\n",
    "\n",
    "# Seperate template into sub-sections USE LOW POWER LMMM\n",
    "section_seperator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation. \n",
    "    Your task is to extract the subsections from {template}.         \n",
    "    Respond with only the template subsections nothing else.\"\"\"),\n",
    "# SPECIFY HOW TO STRUCUTRE SUBSECTIONS SO CAN BE EXTRACTED EASILY\n",
    "    (\"human\", \"{template}\")\n",
    "])\n",
    "\n",
    "# For the hypothetical response generating LLM\n",
    "hypothetical_response_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation. \n",
    "    Your task is to generate a hypothetical response for the following question: {subsection}. \n",
    "    The response should be based on the template {template}, a template for a report on topic {topic}.\n",
    "    \n",
    "    Respond with only the hypothetical response nothing else.\"\"\"),\n",
    "    (\"human\", \"{subsection}, {template}, {topic}\")\n",
    "])\n",
    "# Prompts for each of the sub-section models\n",
    "subsection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation. \n",
    "    Your task is to create a one paragraph {subsection} as part of a report on {topic}.\n",
    "    Use only information on {context} to create the paragraph.\n",
    "    The paragraph should be concise and informative, summarizing the key points relevant to the subsection.     \n",
    "    Respond with only the paragraph for that subsections nothing else.\"\"\"),\n",
    "    (\"human\", \"{subsection}, {context}, {topic}\")\n",
    "])\n",
    "\n",
    "\n",
    "# THIS COULD BE DONE WITHOUT USING AN LLM???\n",
    "# Prompt for the compling model\n",
    "compiling_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation. \n",
    "    Your task is to compile the following subsections {all_subsections}.\n",
    "    The report should be structured according to the template {template}.\n",
    "    The report must match exactly the template structure.\n",
    "    \n",
    "    Respond with only the compiled report nothing else.\"\"\"),\n",
    "    (\"human\", \"{all_subsections}, {template}\")\n",
    "])\n",
    "\n",
    "# Prompt for the checking model\n",
    "checking_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation. \n",
    "    Your task is to check the following subsections {all_subsections} for consistency and completeness.\n",
    "    Also ensure that the report {report} matches the template {template}.\n",
    "    Also ensure that the report is related to the topic {topic}.\n",
    "     \n",
    "    If \n",
    "        1) One of the subsections appears incorrect or incomplete → output ONLY: (**subsection name**), incomplete\n",
    "        2) The report does not match the template → output ONLY: not match\n",
    "        3) The report is not related to the topic → output ONLY: not related\n",
    "        4) Everything is correct → output ONLY: ok\n",
    "     \n",
    "    Respond with only the result of the check (ok, not related, not match, (**subsection name**) incomplete) nothing else.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{all_subsections}, {template}, {report}, {topic}\")\n",
    "])\n",
    "\n",
    "# Prompt for the rewrite subsection model\n",
    "rewrite_subsection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert legal analyst specializing in climate legislation. \n",
    "    Your task is to rewrite the following subsection {subsection} to make it more complete and consistent.\n",
    "    Use the template {template} as a guide for the structure.\n",
    "    The rewritten subsection should be concise and informative, summarizing the key points relevant to the subsection.\n",
    "    \n",
    "    Respond with only the rewritten subsection nothing else.\"\"\"),\n",
    "    (\"human\", \"{subsection}, {template}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932133bb",
   "metadata": {},
   "source": [
    "### 1.2 Prompt Chaining\n",
    "\n",
    "Workflow \n",
    "1. Generate initial template (med model)\n",
    "\n",
    "2. Human template approval (if fail rewrite with higher power model) \n",
    "\n",
    "3. Extract subsections from approved template (low model)\n",
    "\n",
    "4. Generate hypothetical responses for each subsection (med model) \n",
    "    - Retrieve relevant context using hypothetical responses\n",
    "\n",
    "6. Generate actual subsections using retrieved context (high model)\n",
    "\n",
    "7. Compile subsections into full report (low model)\n",
    "\n",
    "8. Quality check (structure, completeness, relevance - if any fail rewrite with higher model) (high model)\n",
    "\n",
    "9. Human final approval with specific feedback collection (high model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.schema import BaseOutputParser\n",
    "import re\n",
    "\n",
    "class ReportWorkflow:\n",
    "    def __init__(self, low_model, med_model, high_model, retriever=None):\n",
    "        self.low_model = low_model\n",
    "        self.med_model = med_model\n",
    "        self.high_model = high_model\n",
    "        self.retriever = retriever\n",
    "        \n",
    "    def _extract_subsections(self, template):\n",
    "        \"\"\"Extract subsections from template - can be done without LLM\"\"\"\n",
    "        # Simple regex-based extraction (replace with LLM if needed)\n",
    "        sections = re.findall(r'Q\\d+:.*?(?=Q\\d+:|$)', template, re.DOTALL)\n",
    "        return [section.strip() for section in sections if section.strip()]\n",
    "    \n",
    "    def _human_approval(self, content, prompt_msg):\n",
    "        \"\"\"Simulate human approval - replace with actual UI\"\"\"\n",
    "        print(f\"\\n{prompt_msg}\")\n",
    "        print(content)\n",
    "        return input(\"Approve? (y/n): \").lower() == 'y'\n",
    "    \n",
    "    def _retrieve_context(self, hypothetical_response):\n",
    "        \"\"\"Retrieve relevant context using hypothetical response\"\"\"\n",
    "        if self.retriever:\n",
    "            return self.retriever.get_relevant_documents(hypothetical_response)\n",
    "        return \"No context available\"  # Placeholder\n",
    "    \n",
    "    def generate_template(self, topic):\n",
    "        \"\"\"Step 1-2: Generate and approve template\"\"\"\n",
    "        chain = template_prompt | self.med_model\n",
    "        template = chain.invoke({\"topic\": topic}).content\n",
    "        \n",
    "        if not self._human_approval(template, \"Review template:\"):\n",
    "            # Retry with higher power model\n",
    "            chain = template_prompt | self.high_model\n",
    "            template = chain.invoke({\"topic\": topic}).content\n",
    "            \n",
    "        return template\n",
    "    \n",
    "    def extract_subsections(self, template):\n",
    "        \"\"\"Step 3: Extract subsections\"\"\"\n",
    "        # Using simple extraction - uncomment below for LLM approach\n",
    "        return self._extract_subsections(template)\n",
    "        \n",
    "        # LLM approach:\n",
    "        # chain = section_seperator_prompt | self.low_model\n",
    "        # result = chain.invoke({\"template\": template}).content\n",
    "        # return result.split('\\n')\n",
    "    \n",
    "    def generate_subsection_content(self, subsection, template, topic):\n",
    "        \"\"\"Step 4-6: Generate hypothetical response, retrieve context, create actual subsection\"\"\"\n",
    "        # Generate hypothetical response\n",
    "        hyp_chain = hypothetical_response_prompt | self.med_model\n",
    "        hypothetical = hyp_chain.invoke({\n",
    "            \"subsection\": subsection,\n",
    "            \"template\": template,\n",
    "            \"topic\": topic\n",
    "        }).content\n",
    "        \n",
    "        # Retrieve context\n",
    "        context = self._retrieve_context(hypothetical)\n",
    "        \n",
    "        # Generate actual subsection\n",
    "        sub_chain = subsection_prompt | self.high_model\n",
    "        return sub_chain.invoke({\n",
    "            \"subsection\": subsection,\n",
    "            \"context\": context,\n",
    "            \"topic\": topic\n",
    "        }).content\n",
    "    \n",
    "    def compile_report(self, subsections, template):\n",
    "        \"\"\"Step 7: Compile subsections into report\"\"\"\n",
    "        chain = compiling_prompt | self.low_model\n",
    "        return chain.invoke({\n",
    "            \"all_subsections\": \"\\n\\n\".join(subsections),\n",
    "            \"template\": template\n",
    "        }).content\n",
    "    \n",
    "    def quality_check_and_fix(self, subsections, template, report, topic):\n",
    "        \"\"\"Step 8: Quality check with potential fixes\"\"\"\n",
    "        check_chain = checking_prompt | self.high_model\n",
    "        \n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            check_result = check_chain.invoke({\n",
    "                \"all_subsections\": \"\\n\\n\".join(subsections),\n",
    "                \"template\": template,\n",
    "                \"report\": report,\n",
    "                \"topic\": topic\n",
    "            }).content.strip()\n",
    "            \n",
    "            if check_result == \"ok\":\n",
    "                return report, subsections\n",
    "            \n",
    "            # Handle different failure cases\n",
    "            if \"incomplete\" in check_result:\n",
    "                # Extract subsection name and rewrite\n",
    "                section_match = re.search(r'\\*\\*(.*?)\\*\\*', check_result)\n",
    "                if section_match:\n",
    "                    section_name = section_match.group(1)\n",
    "                    # Find and rewrite the problematic subsection\n",
    "                    for i, subsection in enumerate(subsections):\n",
    "                        if section_name.lower() in subsection.lower():\n",
    "                            rewrite_chain = rewrite_subsection_prompt | self.high_model\n",
    "                            subsections[i] = rewrite_chain.invoke({\n",
    "                                \"subsection\": subsection,\n",
    "                                \"template\": template\n",
    "                            }).content\n",
    "                            break\n",
    "            \n",
    "            elif check_result in [\"not match\", \"not related\"]:\n",
    "                # Regenerate entire report\n",
    "                report = self.compile_report(subsections, template)\n",
    "        \n",
    "        return report, subsections\n",
    "    \n",
    "    def run_workflow(self, topic):\n",
    "        \"\"\"Main workflow execution\"\"\"\n",
    "        print(f\"Starting report generation for topic: {topic}\")\n",
    "        \n",
    "        # Step 1-2: Generate and approve template\n",
    "        template = self.generate_template(topic)\n",
    "        \n",
    "        # Step 3: Extract subsections\n",
    "        subsections_list = self.extract_subsections(template)\n",
    "        \n",
    "        # Step 4-6: Generate content for each subsection\n",
    "        subsection_contents = []\n",
    "        for subsection in subsections_list:\n",
    "            content = self.generate_subsection_content(subsection, template, topic)\n",
    "            subsection_contents.append(content)\n",
    "        \n",
    "        # Step 7: Compile report\n",
    "        report = self.compile_report(subsection_contents, template)\n",
    "        \n",
    "        # Step 8: Quality check and fix\n",
    "        final_report, final_subsections = self.quality_check_and_fix(\n",
    "            subsection_contents, template, report, topic\n",
    "        )\n",
    "        \n",
    "        # Step 9: Human final approval\n",
    "        if self._human_approval(final_report, \"Final report review:\"):\n",
    "            print(\"Report approved and completed!\")\n",
    "            return final_report\n",
    "        else:\n",
    "            feedback = input(\"Please provide feedback: \")\n",
    "            print(f\"Report rejected. Feedback: {feedback}\")\n",
    "            return None\n",
    "\n",
    "# Usage\n",
    "workflow = ReportWorkflow(\n",
    "    low_model=super_basic_model,\n",
    "    med_model=standard_model, \n",
    "    high_model=larger_context_model,\n",
    "    retriever=None  # Add your retriever here\n",
    ")\n",
    "\n",
    "# Generate report\n",
    "final_report = workflow.run_workflow(\"carbon pricing mechanisms\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
