{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c802840d",
   "metadata": {},
   "source": [
    "## Read the data from the database\n",
    "\n",
    "So it's easier to access the data in case the kernel crashes and had to re-run the codes again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9578b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the table\n",
    "df = pd.read_sql(\"SELECT * FROM climate_policy_radar\", engine)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c3320",
   "metadata": {},
   "source": [
    "# 2. Embeddings generation\n",
    "\n",
    "## 2.1 Load climateBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df15eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_LOCAL_DIR = os.getenv('EMBEDDING_MODEL_LOCAL_DIR')\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL, use_auth_token=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained(EMBEDDING_MODEL, use_auth_token=False)\n",
    "\n",
    "# Save it to a  local_models folder\n",
    "tokenizer.save_pretrained(EMBEDDING_MODEL_LOCAL_DIR)\n",
    "model.save_pretrained(EMBEDDING_MODEL_LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_LOCAL_DIR)\n",
    "model = AutoModel.from_pretrained(EMBEDDING_MODEL_LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c8220",
   "metadata": {},
   "source": [
    "### Checking existing documents' country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT \"document_metadata.geographies\"\n",
    "FROM climate_policy_radar\n",
    "WHERE \"document_metadata.geographies\" IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "geos = pd.read_sql(query, engine)\n",
    "print(geos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f8ea8",
   "metadata": {},
   "source": [
    "## 2.2 Embedding all documents for all countries\n",
    "\n",
    "Generate embeddings for all documents and upload them into the database.\n",
    "\n",
    "A new table is needed, this will be created through the create_table.sql file.So go to create_table.sql and run the query to create the table. Remember to select the Postgres Server at the bottom, and highlight the code and right click to run query. This will create a new table in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure column is string for filtering\n",
    "df[\"document_metadata.geographies\"] = df[\"document_metadata.geographies\"].astype(str)\n",
    "\n",
    "# Extract 3-letter codes like 'ALB', 'DEU', etc.\n",
    "df[\"country_code\"] = df[\"document_metadata.geographies\"].str.extract(r\"\\{(\\w+)\\}\")\n",
    "\n",
    "# Get all unique codes\n",
    "country_codes = df[\"country_code\"].dropna().unique()\n",
    "\n",
    "# Store each country chunk in a dictionary\n",
    "country_chunks = {}\n",
    "\n",
    "for code in tqdm(country_codes, desc=\"Filtering by country\"):\n",
    "    country_chunks[code] = df[df[\"country_code\"] == code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(country_chunks))\n",
    "\n",
    "#how many documents for each country\n",
    "for code, chunk in country_chunks.items():\n",
    "    print(f\"{code}: {len(chunk)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e634b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the \"data\" directory exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Batch size for embedding\n",
    "batch_size = 10000\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "engine = create_engine(os.getenv(\"DB_URL\"))\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Loop through each country's data in the dictionary\n",
    "for code, chunk in tqdm(country_chunks.items(), desc=\"Processing all countries\"):\n",
    "    original_texts = chunk[\"text_block.text\"]\n",
    "    doc_ids = chunk[\"document_id\"]\n",
    "    source_urls = chunk[\"document_metadata.source_url\"]  # ✅ new\n",
    "\n",
    "    num_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_doc_ids = []\n",
    "    all_texts = []\n",
    "    all_urls = []\n",
    "\n",
    "    for i in tqdm(range(num_batches), desc=f\"Embedding {code}\", leave=False):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(texts))\n",
    "\n",
    "        batch_texts = texts.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "        batch_ids = doc_ids.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "        batch_urls = source_urls.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "\n",
    "        mask = batch_texts.apply(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
    "        batch_texts = batch_texts[mask]\n",
    "        batch_ids = batch_ids[mask]\n",
    "        batch_urls = batch_urls[mask]\n",
    "\n",
    "        batch_embeddings = batch_texts.progress_apply(\n",
    "            lambda text: generate_embeddings_for_text(text, model, tokenizer)\n",
    "        )\n",
    "\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        all_doc_ids.extend(batch_ids)\n",
    "        all_texts.extend(batch_texts)\n",
    "        all_urls.extend(batch_urls)\n",
    "\n",
    "    # Upload to DB\n",
    "    for doc_id, embedding, original_text, url in tqdm(\n",
    "        zip(all_doc_ids, all_embeddings, all_texts, all_urls),\n",
    "        total=len(all_doc_ids),\n",
    "        desc=f\"Uploading {code}\"\n",
    "    ):\n",
    "        stmt = text(\"\"\"\n",
    "            INSERT INTO document_embeddings (document_id, country_code, original_text, source_hyperlink, embedding)\n",
    "            VALUES (:document_id, :country_code, :original_text, :source_hyperlink, :embedding)\n",
    "        \"\"\")\n",
    "        session.execute(stmt, {\n",
    "            \"document_id\": doc_id,\n",
    "            \"country_code\": code,\n",
    "            \"original_text\": original_text,\n",
    "            \"source_hyperlink\": url,\n",
    "            \"embedding\": embedding\n",
    "        })\n",
    "\n",
    "\n",
    "    session.commit()\n",
    "\n",
    "\n",
    "print(\"✅ All embeddings and original texts uploaded directly.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
